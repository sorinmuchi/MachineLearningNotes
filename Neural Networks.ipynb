{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Prediction**: Linear Regression\n",
    "\n",
    "**Classification**: Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perceptrons\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![\"The Perceptron\"](NeuralNetworksFiles/perceptron.png \"The Perceptron, a fundamental part of Neural Netowrks\")\n",
    "_The Perceptron, a fundamental part of Neural Networks_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perceptron Formula\n",
    "### Discrete Activation Function\n",
    "$$f(x_1, x_2, x_3, ..., x_m) = \\left\\{\n",
    "                \\begin{array}{ll}\n",
    "                  0 \\text{ if } b + \\sum w_i * x_i < 0\\\\\n",
    "                  1 \\text{ if } b + \\sum w_i * x_i > 0\n",
    "                \\end{array}\n",
    "              \\right.$$\n",
    "              \n",
    "### Continuous Activation Function\n",
    "_Sigmoid Function:_\n",
    "\n",
    "$$ \\sigma(x) = \\frac{1}{1 + e^{-x}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax\n",
    "$$p_i = \\frac{e^{z_i}}{ \\sum_{j=0}^{n} e^{z_j}}$$ where $p_i$ is the probability of class $i$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One Hot Encoding\n",
    "One column per feature, with binary values. [0,1] fature present/not present."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Maximum Likelihood\n",
    "Pick the model that gives the existing labels the highest probability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Entropy\n",
    "Negatives of the logarithms of the predicted probabilities. Smaller is better, because larger predicted probability is better [when the prediction is correct].\n",
    "\n",
    "$$ \\text{Cross Entropy } = - \\sum_{i=1}^{m} y_i ln(p_i) + (1-p_i) ln(1-p_i)$$\n",
    "or\n",
    "$$ H(p,q) = - \\sum_{x} p(x) \\ log \\ q(x) $$\n",
    "\n",
    "## Multi-Class Cross Entropy\n",
    "\n",
    "$$ \\text{ Cross Entropy } = - \\sum_{i=1}^{n} \\sum_{j=1}^{m} y_{ij} \\ ln(p_{ij}) $$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
